{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the data from kaggle sentiment competition : https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/roopal/workspace/datasets/kaggle/sentiment\"\n",
    "data_path_train = os.path.join(data_path, \"train.tsv\")\n",
    "data_path_test = os.path.join(data_path, \"test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(data_path_train, data_path_test):\n",
    "    df_train = pd.read_csv(data_path_train, sep=\"\\t\")\n",
    "    df_test = pd.read_csv(data_path_test, sep=\"\\t\")\n",
    "    \n",
    "    X_train = df_train[\"Phrase\"].tolist()\n",
    "    Y_train = df_train[\"Sentiment\"].tolist()\n",
    "    \n",
    "    X_test = list()\n",
    "    Y_test = list()\n",
    "    \n",
    "    tag2idx = dict(enumerate(sorted(set(df_train[\"Sentiment\"].tolist()))))\n",
    "    \n",
    "    \"\"\"\n",
    "    maybe ignore numbers or treat all numbers as one key <NUM>\n",
    "    \"\"\"\n",
    "    list_vocab = list(set(np.hstack(map(lambda sent: sent.split(), X_train))))\n",
    "    list_vocab = map(lambda wrd: wrd.lower(), list_vocab)\n",
    "    \n",
    "    word2idx = dict()\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    idx_wrd = 1\n",
    "    for word in list_vocab:\n",
    "        idx = word2idx.get(word, None)\n",
    "        \n",
    "        if not idx:\n",
    "            word2idx[word] = idx_wrd\n",
    "            idx_wrd += 1\n",
    "    \n",
    "    #represents the unknown words\n",
    "    word2idx[\"<UNK>\"] = idx_wrd\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, word2idx, tag2idx = get_data(data_path_train, data_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {idx: word for word, idx in word2idx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the vocab file for visualization\n",
    "df_idx_word = pd.DataFrame.from_dict(idx2word, orient=\"index\")\n",
    "df_idx_word.to_csv(path_or_buf=\"train_log/sentiment_lstm_mini_batch_gd/sentiment_lstm_vocab.tsv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060 156060\n"
     ]
    }
   ],
   "source": [
    "# length of train data\n",
    "print len(X_train), len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n"
     ]
    }
   ],
   "source": [
    "# tag2idx\n",
    "print tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocabulary size:', 16533)\n",
      "('Tags # ', 5)\n"
     ]
    }
   ],
   "source": [
    "N = len(X_train)\n",
    "V = len(word2idx)\n",
    "K = len(tag2idx)\n",
    "print (\"vocabulary size:\", V)\n",
    "print (\"Tags # \", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story . 1\n"
     ]
    }
   ],
   "source": [
    "# sample of data\n",
    "print X_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert words to indices in the train/test data\n",
    "def convert_words_to_indices(data):\n",
    "    list_idx_data = list()\n",
    "    for sentence in data:\n",
    "        list_words = list()\n",
    "        for word in sentence.split():\n",
    "            word = word.lower()\n",
    "            list_words.append(word2idx[word])\n",
    "        list_idx_data.append(list_words)\n",
    "    return list_idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = convert_words_to_indices(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229, 343, 3119, 11209, 3651, 5322, 13309, 1800, 11003, 797, 8306, 954, 5322, 5944, 797, 2772, 8306, 954, 5322, 10438, 9015, 7516, 3119, 13463, 4837, 11742, 12641, 7002, 3119, 13463, 10524, 2303, 733, 3119, 229, 6541, 729] 1\n"
     ]
    }
   ],
   "source": [
    "# sample of data\n",
    "print X_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(Mi, Mo):\n",
    "        return np.random.rand(Mi, Mo)/ np.sqrt(Mi+Mo)\n",
    "    \n",
    "    def __init__(self, D, M, V, K, batch_size=2, learning_rate=0.05):\n",
    "        \"\"\"\n",
    "        D: dimensionality of word embeddings\n",
    "        M: size of hidden layer\n",
    "        V: size of vocabulary\n",
    "        K: num of output classes\n",
    "        \"\"\"\n",
    "        self.D = D\n",
    "        self.M = M\n",
    "        self.V = V\n",
    "        self.K = K\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.tf_session = tf.Session()\n",
    "        \n",
    "        with tf.name_scope(\"We\"):\n",
    "            self.We = tf.Variable(tf.random_uniform([self.V, self.D], -0.0001, 0.0001), name=\"We\")\n",
    "            \n",
    "            tf.summary.histogram(\"hist_We\", self.We)\n",
    "        \n",
    "        with tf.name_scope(\"f_gate\"):\n",
    "            self.Wxf = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxf\")\n",
    "            self.Whf = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Whf\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxf\", self.Wxf)\n",
    "            tf.summary.histogram(\"hist_Whf\", self.Whf)\n",
    "                \n",
    "        with tf.name_scope(\"i_gate\"):\n",
    "            self.Wxi = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxi\")\n",
    "            self.Whi = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Whi\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxi\", self.Wxi)\n",
    "            tf.summary.histogram(\"hist_Whi\", self.Whi)\n",
    "            \n",
    "        with tf.name_scope(\"o_gate\"):\n",
    "            self.Wxo = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxo\")\n",
    "            self.Who = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Who\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxo\", self.Wxo)\n",
    "            tf.summary.histogram(\"hist_Who\", self.Who)\n",
    "            \n",
    "        with tf.name_scope(\"c_hat\"):\n",
    "            self.Wxc = tf.Variable(LSTM.init_weights(self.D, self.M), dtype=tf.float32, name=\"Wxc\")\n",
    "            self.Whc = tf.Variable(LSTM.init_weights(self.M, self.M), dtype=tf.float32, name=\"Whc\")\n",
    "\n",
    "            tf.summary.histogram(\"hist_Wxc\", self.Wxc)\n",
    "            tf.summary.histogram(\"hist_Whc\", self.Whc)\n",
    "            \n",
    "        with tf.name_scope(\"biases\"):\n",
    "            self.bi = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bi\")\n",
    "            self.bo = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bo\")\n",
    "            self.bf = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bf\")\n",
    "            self.bc = tf.Variable(tf.zeros(shape=[self.M]), dtype=tf.float32, name=\"bc\")\n",
    "            \n",
    "            tf.summary.histogram(\"hist_bi\", self.bi)\n",
    "            tf.summary.histogram(\"hist_bo\", self.bo)\n",
    "            tf.summary.histogram(\"hist_bf\", self.bf)\n",
    "            tf.summary.histogram(\"hist_bc\", self.bc)\n",
    "            \n",
    "        with tf.name_scope(\"c_0\"):\n",
    "            self.c0 = tf.zeros(shape=[self.M], dtype=tf.float32, name=\"c0\")\n",
    "        \n",
    "        with tf.name_scope(\"h_0\"):\n",
    "            self.h0 = tf.zeros(shape=[self.M], dtype=tf.float32, name=\"h0\")\n",
    "        \n",
    "        self._initial_hidden_cell_states = tf.stack([self.h0, self.c0])\n",
    "        \n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            self.W_op = tf.Variable(LSTM.init_weights(self.M, self.K), dtype=tf.float32, name=\"W_op\")\n",
    "            self.b_op = tf.Variable(tf.zeros(shape=[self.K]), dtype=tf.float32, name=\"b_op\")\n",
    "            \n",
    "            tf.summary.histogram(\"hist_W_op\", self.W_op)\n",
    "            tf.summary.histogram(\"hist_b_op\", self.b_op)\n",
    "            \n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            self.input_seq = tf.placeholder(tf.int32, shape=[None, None], name=\"input_seq\")\n",
    "            self.targets  = tf.placeholder(tf.int32, shape=[None], name=\"targets\")\n",
    "        \n",
    "        self.seq_len = tf.placeholder(tf.int32, shape=[None], name=\"seq_len\")\n",
    "        self.current_batch_size = tf.placeholder(tf.int32, shape=(), name=\"batch_size\")\n",
    "        self.batch_max_len = tf.placeholder(tf.int32, shape=(), name=\"batch_max_len\")\n",
    "        \n",
    "        self.build_graph()\n",
    "\n",
    "        self.save_dir = \"train_log/sentiment_lstm_mini_batch_gd\"\n",
    "        self.model_path = os.path.join(self.save_dir, \"model_sentiment_lstm.ckpt\")\n",
    "        self.emb_path = os.path.join(self.save_dir, \"word_embedding_sentiment_lstm.npy\")\n",
    "        self.saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "        self.add_summary_file_writer()\n",
    "        self.add_summary_embedding()\n",
    "        self.train_writer.add_graph(graph=self.tf_session.graph, global_step=1)\n",
    "        \n",
    "        \n",
    "    def build_graph(self):\n",
    "        input_embeddings = self.get_embeddings(self.input_seq)\n",
    "        \n",
    "        tensor_array_py_x = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False, infer_shape=False, name=\"tensor_array_ho\")\n",
    "\n",
    "        loop_batch_cond = lambda tensor_array_py_x, input_embeddings, idx_sent: tf.less(idx_sent, self.current_batch_size)\n",
    "        batch_py_x, _, _ = tf.while_loop(\n",
    "            loop_batch_cond, self.loop_batch, (tensor_array_py_x, input_embeddings, 0), parallel_iterations=5, name=\"loop_\"\n",
    "        )\n",
    "\n",
    "        self.py_x = batch_py_x.concat()\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.py_x, labels=self.targets)           \n",
    "            self.loss = tf.divide(tf.reduce_sum(loss), tf.cast(self.current_batch_size, tf.float32))\n",
    "        \n",
    "        with tf.name_scope(\"train_op\"):\n",
    "            trainables = tf.trainable_variables()\n",
    "            \n",
    "            grads = tf.gradients(self.loss, trainables)\n",
    "            \n",
    "            grads, _ = tf.clip_by_global_norm(grads, clip_norm=1)\n",
    "            grad_var_pairs = zip(grads, trainables)\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "            self.train_op = opt.apply_gradients(grad_var_pairs)\n",
    "        \n",
    "    def add_summary_file_writer(self):\n",
    "        print \"Creating FileWriter\"\n",
    "        self.train_writer = tf.summary.FileWriter(self.save_dir , graph=self.tf_session.graph)\n",
    "        \n",
    "    def add_summary_embedding(self):\n",
    "        \"\"\"\n",
    "        refer https://www.tensorflow.org/how_tos/embedding_viz/\n",
    "        \"\"\"\n",
    "        print \"Creating Embedding Projections\"\n",
    "        config = projector.ProjectorConfig()\n",
    "        embedding = config.embeddings.add()\n",
    "        embedding.tensor_name = self.We.name\n",
    "        embedding.metadata_path = \"sentiment_lstm_vocab.tsv\"\n",
    "        projector.visualize_embeddings(self.train_writer, config)\n",
    "    \n",
    "\n",
    "    def get_embeddings(self, idx_input_seq):\n",
    "        return tf.nn.embedding_lookup(self.We, idx_input_seq)\n",
    "    \n",
    "    def _recurrence(self, previous_hidden_memory_tuple, x_t):\n",
    "        h_t_minus_1, c_t_minus_1 = tf.unstack(previous_hidden_memory_tuple)\n",
    "        \n",
    "        x_t = tf.reshape(x_t, [1, self.D])\n",
    "        h_t_minus_1 = tf.reshape(h_t_minus_1,[1, self.M])\n",
    "        c_t_minus_1 = tf.reshape(c_t_minus_1,[1, self.M])\n",
    "        \n",
    "        f_t = tf.nn.sigmoid(\n",
    "            tf.matmul(x_t, self.Wxf) + tf.matmul(h_t_minus_1, self.Whf) + self.bf\n",
    "        )\n",
    "        \n",
    "        i_t = tf.nn.sigmoid(\n",
    "            tf.matmul(x_t, self.Wxi) + tf.matmul(h_t_minus_1, self.Whi) + self.bi\n",
    "        )\n",
    "        \n",
    "        c_hat_t = tf.nn.tanh(\n",
    "            tf.matmul(x_t, self.Wxc) + tf.matmul(h_t_minus_1, self.Whc) + self.bc\n",
    "        )\n",
    "        \n",
    "        c_t = (f_t * c_t_minus_1) + (i_t * c_hat_t)\n",
    "        \n",
    "        o_t = tf.nn.sigmoid(\n",
    "            tf.matmul(x_t, self.Wxo) + tf.matmul(h_t_minus_1, self.Who) + self.bo\n",
    "        )\n",
    "        \n",
    "        h_t = o_t * tf.nn.tanh(c_t)\n",
    "        \n",
    "        h_t = tf.reshape(h_t, [self.M])\n",
    "        c_t = tf.reshape(c_t, [self.M])\n",
    "        \n",
    "        return tf.stack([h_t, c_t])\n",
    "    \n",
    "    def loop_batch(self, tensor_array_py_x, input_embeddings, idx_sent):\n",
    "        hidden_cell_states = tf.scan(\n",
    "            fn=self._recurrence, elems=tf.gather(input_embeddings, idx_sent), initializer=self._initial_hidden_cell_states, name=\"hidden_states\"\n",
    "        )\n",
    "        \n",
    "        h_t, c_t = tf.unstack(hidden_cell_states, axis=1)\n",
    "\n",
    "        h_t_last = tf.reshape(h_t[-1, :], [1, self.M])\n",
    "\n",
    "        py_x = tf.matmul(h_t_last, self.W_op) + self.b_op\n",
    "        \n",
    "        tensor_array_py_x = tensor_array_py_x.write(idx_sent, py_x)\n",
    "        idx_sent = tf.add(idx_sent, 1)\n",
    "        \n",
    "        return tensor_array_py_x, input_embeddings, idx_sent\n",
    "    \n",
    "    \n",
    "    def fit(self, X, Y, lr=1e-2, epochs=500):\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        print (\"Initializing global variables\")\n",
    "        self.tf_session.run(tf.global_variables_initializer())\n",
    "        print (\"# of trainable var outside: \" + str(len(tf.trainable_variables())))\n",
    "        \n",
    "        net_epoch_step_idx = 0\n",
    "        \n",
    "        num_samples = len(X)\n",
    "        costs = list()\n",
    "        for idx_epoch in xrange(epochs):\n",
    "            cost_epoch = 0\n",
    "            accuracy_epoch = list()\n",
    "            \n",
    "            X_train, Y_train = shuffle(X, Y, n_samples=num_samples)\n",
    "            \n",
    "            net_epoch_step_idx = num_samples * idx_epoch\n",
    "            current_idx_sent = 0\n",
    "        \n",
    "            while current_idx_sent < len(X_train):\n",
    "                print (\"----------------------------------------------------\")\n",
    "                print (\"epoch: {}, sentence: {}\".format(idx_epoch, current_idx_sent))\n",
    "                \n",
    "                seq_len = list()\n",
    "                \n",
    "                targets = Y_train[current_idx_sent: current_idx_sent + self.batch_size]\n",
    "                \n",
    "                x = X_train[current_idx_sent: current_idx_sent + self.batch_size]\n",
    "                max_len = max([len(sentence) for sentence in x])\n",
    "                                \n",
    "                input_seq = list()\n",
    "                for index, sentence in enumerate(x):\n",
    "                    seq_len.append(len(sentence))\n",
    "                    padded_sentence = list(np.pad(sentence, (0, max_len-len(sentence)), 'constant', constant_values=0))\n",
    "                    input_seq.append(padded_sentence)\n",
    "                \n",
    "                current_batch_size = len(seq_len)\n",
    "                \n",
    "                net_epoch_step_idx += current_batch_size\n",
    "                \n",
    "                feed_dict={\n",
    "                    self.input_seq: input_seq, \n",
    "                    self.targets: targets,\n",
    "                    self.current_batch_size: current_batch_size,\n",
    "                    self.seq_len: seq_len,\n",
    "                    self.batch_max_len: max_len\n",
    "                }\n",
    "                \n",
    "                self.tf_session.run(self.train_op, feed_dict=feed_dict)\n",
    "                                \n",
    "                py_x, loss, We = self.tf_session.run([self.py_x, self.loss, self.We], feed_dict=feed_dict)\n",
    "                \n",
    "                pred = np.argmax(py_x, axis=1)\n",
    "                print \"Y: \", targets\n",
    "                print \"Prediction: \", pred\n",
    "                \n",
    "                accuracy = 0\n",
    "                for y, y_ in zip(targets, pred):\n",
    "                    if y==y_:\n",
    "                        accuracy += 1 \n",
    "                accuracy = float(accuracy)/len(pred)\n",
    "        \n",
    "                print \"Accuracy/Batch # {}\".format(current_idx_sent), accuracy\n",
    "                print \"Loss/Batch # {}\".format(current_idx_sent), loss\n",
    "                cost_epoch += loss\n",
    "                \n",
    "                accuracy_epoch.append(accuracy)\n",
    "                \n",
    "                loss_step = tf.Summary(\n",
    "                    value=[\n",
    "                        tf.Summary.Value(tag=\"loss_per_mini_batch\", simple_value=loss),\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                accuracy_step = tf.Summary(\n",
    "                    value=[\n",
    "                        tf.Summary.Value(tag=\"accuracy_per_mini_batch\", simple_value=accuracy),\n",
    "                    ]\n",
    "                )\n",
    "                    \n",
    "                self.train_writer.add_summary(loss_step, net_epoch_step_idx)\n",
    "                self.train_writer.add_summary(accuracy_step, net_epoch_step_idx)\n",
    "                \n",
    "                if net_epoch_step_idx%1000 == 0:\n",
    "                    self.save_model(step=net_epoch_step_idx)\n",
    "                    summary = self.tf_session.run(tf.summary.merge_all())\n",
    "                    self.train_writer.add_summary(summary, idx_epoch)\n",
    "                    self.train_writer.flush()\n",
    "                \n",
    "                current_idx_sent += self.batch_size\n",
    "                \n",
    "            costs.append(cost_epoch)\n",
    "            \n",
    "            print (\"---------\")\n",
    "            print (\"Cost at epoch {} is {}\".format(idx_epoch, cost_epoch))\n",
    "\n",
    "            print (\"Accuracy at epoch {} is {}\".format(idx_epoch, np.mean(accuracy_epoch)))\n",
    "            print (\"---------\")\n",
    "            \n",
    "            self.save_model(step=net_epoch_step_idx)\n",
    "            \n",
    "            summary_cost_epoch = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(tag=\"loss/epoch\", simple_value=cost_epoch),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            summary_accuracy_epoch = tf.Summary(\n",
    "                value=[\n",
    "                    tf.Summary.Value(tag=\"accuracy/epoch\", simple_value=np.mean(accuracy_epoch)),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.train_writer.add_summary(summary_cost_epoch, idx_epoch)\n",
    "            self.train_writer.add_summary(summary_accuracy_epoch, idx_epoch)\n",
    "            summary = self.tf_session.run(tf.summary.merge_all())\n",
    "            self.train_writer.add_summary(summary, idx_epoch)\n",
    "            self.train_writer.flush()\n",
    "\n",
    "    def save_model(self, step=1):\n",
    "        self.save_embedding_matrix()\n",
    "        print \"saving model for step\", step, \"to\", self.model_path\n",
    "        self.saver.save(self.tf_session, self.model_path, step)\n",
    "        \n",
    "    def save_embedding_matrix(self):\n",
    "        np.save(self.emb_path, self.We.eval(self.tf_session))\n",
    "        \n",
    "    def close_session(self):\n",
    "        self.tf_session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FileWriter\n",
      "Creating Embedding Projections\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(300, 10, V, K, batch_size=10, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    lstm.fit(X_train[:100], Y_train[:100])\n",
    "finally:\n",
    "    print (\"Closing Session\")\n",
    "    lstm.close_session()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
